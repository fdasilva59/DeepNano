{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow 2.0 tests on Nvidia Jetson Nano and Google Coral TPU Edge\n",
    "\n",
    "** WORK in progress **"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# MIT License\n",
    "#\n",
    "# Inspired by Tensorflow tutorials \"Basic classification: Classify images of clothing\"\n",
    "#    Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
    "#    https://tensorflow.org/tutorials/keras/classification\n",
    "#\n",
    "# This notebook by Fabien Da Silva (github @fdasilva59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.6.8 (/opt/local/virtual-env)\n",
      "Tensorflow:2.0.0\n",
      "Available GPU: /device:GPU:0\n",
      "Built with CUDA: True\n",
      "GPU details: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "USB TPU Edge device: ('/sys/bus/usb/devices/2-1.3',)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from edgetpu.basic import edgetpu_utils\n",
    "from platform import python_version\n",
    "\n",
    "\n",
    "# Enable device placement logging\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Check configuration\n",
    "print(\"Python: {} ({})\".format(python_version(), sys.prefix))\n",
    "print(\"Tensorflow:{}\".format(tf.version.VERSION))\n",
    "if tf.test.is_gpu_available():\n",
    "    gpu = tf.config.experimental.list_physical_devices('GPU')[0]\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"Available GPU: {}\".format(tf.test.gpu_device_name()))\n",
    "    print(\"Built with CUDA: {}\".format(tf.test.is_built_with_cuda()))\n",
    "    print(\"GPU details:\" , gpu)\n",
    "\n",
    "tpu = edgetpu_utils.ListEdgeTpuPaths(edgetpu_utils.EDGE_TPU_STATE_NONE)\n",
    "if len(tpu) >0:\n",
    "    print(\"USB TPU Edge device: {}\".format(tpu))\n",
    "else:\n",
    "    print(\"No USB TPU Edge device found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train a basic Tensorflow model (using Keras API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset download and preparation\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_img, train_labels), (test_img, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "class_names = ['t-shirt/top', 'trouser', 'pullover', 'dress', ' coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot' ]\n",
    "\n",
    "# Add a channel dimension [batch, width, height, channel]\n",
    "train_img = train_img.reshape(train_img.shape[0], train_img.shape[1], train_img.shape[2], 1)\n",
    "test_img = test_img.reshape(test_img.shape[0], test_img.shape[1], test_img.shape[2], 1)\n",
    "\n",
    "# Normalize values\n",
    "train_img = train_img / 255.0\n",
    "test_img = test_img / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 11, 11, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 233,802\n",
      "Trainable params: 233,226\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build and Train a basic model in Keras\n",
    "\n",
    "mode='ADVANCED' #'BASIC'\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "if (mode=='BASIC'):\n",
    "    \n",
    "    # basic model : small fully conected neural network (100K params)\n",
    "    model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
    "    model.add(keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "else:\n",
    "    \n",
    "    # Advanced model : small convolutional neural network (230K params)\n",
    "    model.add(keras.layers.InputLayer(input_shape=(28,28,1)))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation=None))\n",
    "    model.add(keras.layers.BatchNormalization(axis=-1))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation=None))\n",
    "    model.add(keras.layers.BatchNormalization(axis=-1))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    \n",
    "    model.add(keras.layers.Dense(128, activation=None))\n",
    "    model.add(keras.layers.BatchNormalization(axis=-1))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Dropout(rate=0.3))\n",
    "              \n",
    "    model.add(keras.layers.Dense(64, activation=None))\n",
    "    model.add(keras.layers.BatchNormalization(axis=-1))\n",
    "    model.add(keras.layers.Activation('relu'))          \n",
    "    model.add(keras.layers.Dropout(rate=0.3))\n",
    "              \n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RangeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op DatasetCardinality in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_1778 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_distributed_function_2455 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "60000/60000 [==============================] - 66s 1ms/sample - loss: 0.6423 - accuracy: 0.7895\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 49s 814us/sample - loss: 0.3931 - accuracy: 0.8651\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 48s 807us/sample - loss: 0.3303 - accuracy: 0.8855\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 49s 817us/sample - loss: 0.3033 - accuracy: 0.8942\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 49s 813us/sample - loss: 0.2736 - accuracy: 0.9026- loss: 0.2735 - accuracy: 0.\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 49s 821us/sample - loss: 0.2555 - accuracy: 0.9105\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 49s 816us/sample - loss: 0.2408 - accuracy: 0.9154\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 49s 820us/sample - loss: 0.2269 - accuracy: 0.9184\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 50s 836us/sample - loss: 0.2093 - accuracy: 0.9264\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 51s 843us/sample - loss: 0.2013 - accuracy: 0.9280\n",
      "Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "\n",
      "--- Elapsed Time: 510.092234 sec (Batch size = 64) ---\n"
     ]
    }
   ],
   "source": [
    "# Train the model (Note: The Jetson Nano is optimized for inference, not for training...)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "start_time=time.time()\n",
    "model.fit(train_img, train_labels, epochs=epochs, batch_size=batch_size)\n",
    "print(\"\\n--- Elapsed Time: {:.6f} sec (Batch size = {}) ---\".format(time.time() - start_time, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_distributed_function_30858 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "\n",
      "Accuracy on the test dataset 0.9052\n",
      "\n",
      "--- Elapsed Time: 4.468205 sec (Batch size = 64) ---\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "\n",
    "start_time=time.time()\n",
    "test_loss, test_acc = model.evaluate(test_img, test_labels, batch_size=batch_size, verbose=0)\n",
    "print(\"\\nAccuracy on the test dataset {:3.5}\".format(test_acc))\n",
    "print(\"\\n--- Elapsed Time: {:.6f} sec (Batch size = {}) ---\".format(time.time() - start_time, batch_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with Tensorflow (Keras API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RangeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_distributed_function_31508 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "\n",
      "--- Elapsed Time: 10.756454 sec (Batch size = 64) ---\n"
     ]
    }
   ],
   "source": [
    "# Perform inference on the CPU cores\n",
    "\n",
    "start_time=time.time()\n",
    "with tf.device('/CPU:0'):\n",
    "    predictions = model.predict(test_img, batch_size=batch_size, verbose=0)\n",
    "print(\"\\n--- Elapsed Time: {:.6f} sec (Batch size = {}) ---\".format(time.time() - start_time, batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RangeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_distributed_function_31508 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "\n",
      "--- Elapsed Time: 3.925985 sec (Batch size = 64) ---\n"
     ]
    }
   ],
   "source": [
    "# Perform inference on the Jetson Nano GPU\n",
    "\n",
    "start_time=time.time()\n",
    "with tf.device('/GPU:0'):\n",
    "    predictions = model.predict(test_img, batch_size=batch_size, verbose=0)\n",
    "print(\"\\n--- Elapsed Time: {:.6f} sec (Batch size = {}) ---\".format(time.time() - start_time, batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predition: ankle boot       label: ankle boot     \n",
      "predition: pullover         label: pullover       \n",
      "predition: trouser          label: trouser        \n",
      "predition: trouser          label: trouser        \n",
      "predition: shirt            label: shirt          \n"
     ]
    }
   ],
   "source": [
    "# Display some inference results\n",
    "for i in range(5):\n",
    "    print(\"predition: {:15}  label: {:15}\".format(class_names[np.argmax(predictions[i])], \n",
    "                                          class_names[test_labels[i]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op __inference_initialize_variables_31999 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32012 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32023 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32062 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32133 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32173 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32182 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32195 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32206 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32245 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32316 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32356 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32365 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32409 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32486 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32563 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32640 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32770 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32779 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32875 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32884 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32895 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32904 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32918 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32929 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32963 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32972 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_32996 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_33010 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_33030 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_33041 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_33075 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_33084 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_33108 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_33122 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_33143 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_33154 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_33161 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_33292 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_33519 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "WARNING:tensorflow:From /opt/local/virtual-env/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Executing op StringJoin in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ShardedFilename in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op SaveV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op SaveV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Pack in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MergeV2Checkpoints in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "INFO:tensorflow:Assets written to: ./models/assets\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_distributed_function_39564 in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Export the model to Tensorflow Saved Model format\n",
    "model.save('./models', save_format='tf')\n",
    "\n",
    "# Verify that the Saved Model is ok\n",
    "new_model = keras.models.load_model('./models')\n",
    "new_predictions = new_model.predict(test_img, batch_size=batch_size, verbose=0)\n",
    "\n",
    "np.testing.assert_allclose(predictions, new_predictions, rtol=1e-6, atol=1e-6)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sys']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "%who_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the Tensorflow Saved Model into Tensorflow Lite models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload Tensorflow \n",
    "\n",
    "import time\n",
    "import pprint\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from edgetpu.basic import edgetpu_utils\n",
    "from tensorflow.lite.python.interpreter import load_delegate\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Dataset reload & preparation\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_img, train_labels), (test_img, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "class_names = ['t-shirt/top', 'trouser', 'pullover', 'dress', ' coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot' ]\n",
    "\n",
    "# Add a channel dimension [batch, width, height, channel]\n",
    "train_img = train_img.reshape(train_img.shape[0], train_img.shape[1], train_img.shape[2], 1)\n",
    "test_img = test_img.reshape(test_img.shape[0], test_img.shape[1], test_img.shape[2], 1)\n",
    "\n",
    "# Normalize values\n",
    "train_img = train_img / 255.0\n",
    "test_img = test_img / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op TensorSliceDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Equal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalAnd in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Select in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AnonymousRandomSeedGenerator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ShuffleDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op BatchDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TakeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op OptimizeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op IteratorGetNextSync in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DeleteRandomSeedGenerator in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "240048"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF Lite with Weight Quantization (for CPU/TPU inference)\n",
    "\n",
    "# Define a Representative dataset generator to measure the dynamic  \n",
    "# range of activations and inputs during the quantization process\n",
    "\n",
    "def representative_dataset_gen():\n",
    "\n",
    "    representative_dataset = tf.data.Dataset.from_tensor_slices(train_img)\n",
    "    representative_dataset = representative_dataset.shuffle(buffer_size=1000, seed=42)\n",
    "    representative_dataset = representative_dataset.batch(1)\n",
    "    \n",
    "    for representative_input in representative_dataset.take(100):\n",
    "        yield [tf.cast(representative_input, dtype=tf.float32) ]\n",
    "\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('./models')\n",
    "\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]\n",
    "\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.target_spec.supported_types = [tf.int8]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(\"./models/fashion_mnsit_quant_INT8.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "933780"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF Lite without Weight Quantization (for CPU inference)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('./models')\n",
    "tflite_model = converter.convert()\n",
    "open(\"./models/fashion_mnsit.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "239592"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF Lite with Weight Quantization (for CPU inference)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('./models')\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT] \n",
    "tflite_model = converter.convert()\n",
    "open(\"./models/fashion_mnsit_quant.tflite\", \"wb\").write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "239592"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF Lite with Weight Quantization (for CPU inference)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('./models')\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE] \n",
    "tflite_model = converter.convert()\n",
    "open(\"./models/fashion_mnsit_quant_size.tflite\", \"wb\").write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "239592"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF Lite with Weight Quantization (for CPU inference)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('./models')\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY] \n",
    "tflite_model = converter.convert()\n",
    "open(\"./models/fashion_mnsit_quant_latency.tflite\", \"wb\").write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309K ./models/fashion_mnsit_quant_INT8_edgetpu.tflite\n",
      "235K ./models/fashion_mnsit_quant_INT8.tflite\n",
      "234K ./models/fashion_mnsit_quant_latency.tflite\n",
      "234K ./models/fashion_mnsit_quant_size.tflite\n",
      "234K ./models/fashion_mnsit_quant.tflite\n",
      "912K ./models/fashion_mnsit.tflite\n"
     ]
    }
   ],
   "source": [
    "# List TFLite models with size\n",
    "\n",
    "! ls -lh ./models/*.tflite | awk '{print $5,$9}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with Tensorflow Lite ** Work in Progress **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TflModel:\n",
    "\n",
    "    def __init__(self, model=None, target_tpu=False, batch_size=1):\n",
    "        self.model = model\n",
    "        self.target_tpu = target_tpu\n",
    "        self.batch_size = batch_size \n",
    "        self.interpreter = None\n",
    "        self.input_details = None\n",
    "        self.output_details = None\n",
    "        \n",
    "        if model is not None:\n",
    "            self.load(model, target_tpu, batch_size)\n",
    "        \n",
    "            \n",
    "    def load(self, model, target_tpu=False, batch_size=1):\n",
    "        \"\"\"  \n",
    "        Load a Tensorflow Lite model and return\n",
    "\n",
    "            model: Tensorflow Lite model (full path to)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Load TFLite model\n",
    "        \n",
    "        if target_tpu:\n",
    "            print(\"Using tf.lite with TPU delegate\")\n",
    "            self.interpreter = tf.lite.Interpreter(model,\n",
    "                                                   experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\n",
    "        else:\n",
    "            self.interpreter = tf.lite.Interpreter(model)\n",
    "        \n",
    "        # Resize the model's input to match the batch_size\n",
    "        input_shape = self.interpreter.get_input_details()[0]['shape']\n",
    "        input_index = self.interpreter.get_input_details()[0]['index']\n",
    "        input_shape[0] = batch_size\n",
    "        self.interpreter.resize_tensor_input(input_index, input_shape)\n",
    "\n",
    "        # Get input/output tensor's details\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "        \n",
    "        # Allocate the tensors\n",
    "        self.interpreter.allocate_tensors()\n",
    "\n",
    "        # Display information about the model's input/output\n",
    "        print(\"\\nModel's Input details:\")\n",
    "        pprint.pprint(self.input_details)\n",
    "        print(\"\\nModel's Output details:\")\n",
    "        pprint.pprint(self.output_details)\n",
    "        print(\"\")\n",
    "\n",
    "    \n",
    "    def prepare_data(self, input_data):\n",
    "        \"\"\" Prepare input data \"\"\" \n",
    "        \n",
    "        # Working with Numpy arrays\n",
    "        if isinstance(input_data , np.ndarray):\n",
    "           \n",
    "            # Add batch dim if necessary\n",
    "            if len(input_data.shape) == len(self.input_details[0]['shape']) -1 :\n",
    "                input_data = np.expand_dims(input_data, axis=0)   \n",
    "        \n",
    "            # Cast input_data type according to the model's input\n",
    "            input_data = input_data.astype(self.input_details[0]['dtype']) \n",
    "           \n",
    "         # Working with Tensors   \n",
    "        elif isinstance(input_data , tf.Tensor):\n",
    "\n",
    "            # Add batch dim if necessary\n",
    "            if len(input_data.shape) == len(self.input_details[0]['shape']) -1 : \n",
    "                input_data = tf.expand_dims(test_img[0], axis=0)\n",
    "            \n",
    "            # Cast input_data type according to the model's input\n",
    "            input_data = tf.dtypes.cast(input_data, self.input_details[0]['dtype'])\n",
    "            \n",
    "        # Ensure that the input data shape match the model's input shape\n",
    "        assert np.array(input_data.shape).all() == self.input_details[0]['shape'].all(), \\\n",
    "                \"input_data shape {} does not match model input shape {}\".format(input_data.shape, self.input_details[0]['shape'])\n",
    "        \n",
    "        return input_data\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, input_data):   \n",
    "\n",
    "        # Perform inference on the input data\n",
    "        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)\n",
    "        start_time = time.time()\n",
    "        self.interpreter.invoke()\n",
    "\n",
    "        # The function `get_tensor()` returns a copy of the tensor data.\n",
    "        # Use `tensor()` in order to get a pointer to the tensor.\n",
    "        output_data = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
    "\n",
    "        # Do we need this ?\n",
    "        # If the model is quantized (uint8 data), then dequantize the results\n",
    "        #if output_details['dtype'] == np.uint8:\n",
    "\n",
    "        #    print(ou)\n",
    "\n",
    "        #    scale, zero_point = output_details['quantization']\n",
    "        #    output = scale * (output - zero_point)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        return output_data, elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fashion_mnsit_inference(model, target_tpu=False, batch_size=64, batch_limit=None):\n",
    "\n",
    "    print(\"Model:\", model)\n",
    "    print(\"Batch size:\", batch_size)\n",
    "    \n",
    "    batch = 1\n",
    "    total_accuracy = 0\n",
    "\n",
    "    # Load Tensorflow Lite Model\n",
    "    fashion_mnsit = TflModel(model, target_tpu=target_tpu, batch_size=batch_size)\n",
    " \n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_img, test_labels))\n",
    "    test_dataset = test_dataset.batch(batch_size, drop_remainder=True)\n",
    "        \n",
    "    for batch_input_data, batch_labels in test_dataset:\n",
    "\n",
    "        # Check and prepare the input dataset\n",
    "        input_data = fashion_mnsit.prepare_data(batch_input_data)\n",
    "\n",
    "        # Perform Inference\n",
    "        raw_predictions , elapsed_time = fashion_mnsit.predict(input_data)   # [batch, classes]\n",
    "        \n",
    "        # Return the indices that gives the prdictions sorted values\n",
    "        predictions = tf.argsort(raw_predictions, axis =-1, direction='DESCENDING')  # [batch, sorted classes]\n",
    "        \n",
    "        # The first colum contains the prediction with highest confidence\n",
    "        predictions = tf.gather(predictions, indices=[0], axis=1) # [batch, 1]\n",
    "        \n",
    "        # Reshape the predictions tensor like the labels tensor\n",
    "        predictions = tf.transpose(predictions)   # [1, batch]\n",
    "        predictions = tf.squeeze(predictions)     # [ batch ]\n",
    "        \n",
    "        # Check if the predictions are correct ->  [ batch ] array of 0/1 values \n",
    "        correct_predictions = (tf.cast(tf.cast(predictions, tf.int32) == tf.cast(batch_labels, tf.int32), tf.int32 )   )\n",
    "        \n",
    "         # Compute accuracy\n",
    "        absolute_accuracy = tf.reduce_sum(correct_predictions)    \n",
    "        accuracy = absolute_accuracy / batch_size * 100\n",
    "        total_accuracy += absolute_accuracy \n",
    "        print(\"batch {:3} (batch size {:3}): accuracy={:3.2f}% - Global Accuracy={:3.2f}% (Inference time {:.5f} sec - {:3.2f} sec/sample)\"\\\n",
    "              .format(batch, batch_size, accuracy, (total_accuracy/(batch*len(batch_labels))*100), elapsed_time, elapsed_time/batch_size*1000000))\n",
    "        \n",
    "        # Early stop\n",
    "        if (batch is not None) and batch == batch_limit:\n",
    "            print(\"Early stop after {} batch(es)\".format(batch))\n",
    "            break\n",
    "        \n",
    "        # Next Batch\n",
    "        batch +=1\n",
    "        \n",
    "batch_size = 128        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge TPU Compiler version 2.0.267685300\n",
      "\n",
      "Model compiled successfully in 211 ms.\n",
      "\n",
      "Input model: ./models/fashion_mnsit_quant_INT8.tflite\n",
      "Input size: 234.42KiB\n",
      "Output model: ./models/fashion_mnsit_quant_INT8_edgetpu.tflite\n",
      "Output size: 308.84KiB\n",
      "On-chip memory available for caching model parameters: 7.95MiB\n",
      "On-chip memory used for caching model parameters: 234.75KiB\n",
      "Off-chip memory used for streaming uncached model parameters: 0.00B\n",
      "Number of Edge TPU subgraphs: 1\n",
      "Total number of operations: 10\n",
      "Operation log: ./models/fashion_mnsit_quant_INT8_edgetpu.log\n",
      "\n",
      "Model successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.\n",
      "Number of operations that will run on Edge TPU: 8\n",
      "Number of operations that will run on CPU: 2\n",
      "\n",
      "Operator                       Count      Status\n",
      "\n",
      "MAX_POOL_2D                    2          Mapped to Edge TPU\n",
      "QUANTIZE                       1          Operation is otherwise supported, but not mapped due to some unspecified limitation\n",
      "CONV_2D                        1          Mapped to Edge TPU\n",
      "DEPTHWISE_CONV_2D              1          Mapped to Edge TPU\n",
      "DEQUANTIZE                     1          Operation is working on an unsupported data type\n",
      "SOFTMAX                        1          Mapped to Edge TPU\n",
      "FULLY_CONNECTED                3          Mapped to Edge TPU\n"
     ]
    }
   ],
   "source": [
    "!edgetpu_compiler -s -o ./models ./models/fashion_mnsit_quant_INT8.tflite "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WORK IN PROGRESS\n",
    "\n",
    "See: \n",
    "- https://www.tensorflow.org/lite/performance/post_training_quantization\n",
    "- https://www.tensorflow.org/lite/performance/post_training_integer_quant\n",
    "- https://coral.withgoogle.com/docs/edgetpu/tflite-python/\n",
    "\n",
    "While the tensorflow 2.0 documentation has greatly inproved and provide great guides, it looks like some of them not fully up-to-date for Tensorflow 2.0. For example the `converter.inference_input_type` and `converter.inference_output_type` have been removed from the 2.0 API)\n",
    "\n",
    "Open questions:\n",
    "- When comiling the TFLite model for the TPU Edge, is it normal teh quantize/dequnatize operation will remain to be executed on the CPU ?\n",
    "- When running the model on the TPU, why the accuracy is that bad ? (while being OK when run on the Jetson Nano GPU)\n",
    "- Why `self.interpreter.get_input_details()[0]['quantization']` value is `(0.0, 0)`? Same question for `self.interpreter.get_output_details()[0]['quantization']` ?\n",
    "- What about the maturity of tf.lite in TF 2.0 ? What about the maturity of TF 2.0 with the TPU Edge\n",
    "\n",
    "\n",
    "Unsoved Problem:\n",
    "- Unable to build Tennsorlow Lite runtime for Python 3.6 (navively or via cross-compilation)\n",
    "\n",
    "TODO:\n",
    "- Check Tensor RT \n",
    "- Example with OpenCV2/PiCam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ./models/fashion_mnsit_quant_INT8.tflite\n",
      "Batch size: 128\n",
      "\n",
      "Model's Input details:\n",
      "[{'dtype': <class 'numpy.float32'>,\n",
      "  'index': 19,\n",
      "  'name': 'input_1',\n",
      "  'quantization': (0.0, 0),\n",
      "  'shape': array([128,  28,  28,   1], dtype=int32)}]\n",
      "\n",
      "Model's Output details:\n",
      "[{'dtype': <class 'numpy.float32'>,\n",
      "  'index': 20,\n",
      "  'name': 'Identity',\n",
      "  'quantization': (0.0, 0),\n",
      "  'shape': array([ 1, 10], dtype=int32)}]\n",
      "\n",
      "Executing op TensorSliceDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op BatchDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op OptimizeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op IteratorGetNextSync in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Shape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op StridedSlice in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op TopKV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op GatherV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Transpose in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Squeeze in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Equal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sum in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RealDiv in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "batch   1 (batch size 128): accuracy=89.84% - Global Accuracy=89.84% (Inference time 0.09828 sec - 767.79 sec/sample)\n",
      "batch   2 (batch size 128): accuracy=92.19% - Global Accuracy=91.02% (Inference time 0.09313 sec - 727.57 sec/sample)\n",
      "batch   3 (batch size 128): accuracy=95.31% - Global Accuracy=92.45% (Inference time 0.08930 sec - 697.64 sec/sample)\n",
      "Early stop after 3 batch(es)\n"
     ]
    }
   ],
   "source": [
    "fashion_mnsit_inference(\"./models/fashion_mnsit_quant_INT8.tflite\", batch_size=batch_size, batch_limit=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ./models/fashion_mnsit_quant_INT8_edgetpu.tflite\n",
      "Batch size: 128\n",
      "Using tf.lite with TPU delegate\n",
      "\n",
      "Model's Input details:\n",
      "[{'dtype': <class 'numpy.float32'>,\n",
      "  'index': 2,\n",
      "  'name': 'input_1',\n",
      "  'quantization': (0.0, 0),\n",
      "  'shape': array([128,  28,  28,   1], dtype=int32)}]\n",
      "\n",
      "Model's Output details:\n",
      "[{'dtype': <class 'numpy.float32'>,\n",
      "  'index': 3,\n",
      "  'name': 'Identity',\n",
      "  'quantization': (0.0, 0),\n",
      "  'shape': array([ 1, 10], dtype=int32)}]\n",
      "\n",
      "batch   1 (batch size 128): accuracy=7.81% - Global Accuracy=7.81% (Inference time 0.05433 sec - 424.43 sec/sample)\n",
      "batch   2 (batch size 128): accuracy=14.84% - Global Accuracy=11.33% (Inference time 0.05926 sec - 462.95 sec/sample)\n",
      "batch   3 (batch size 128): accuracy=7.81% - Global Accuracy=10.16% (Inference time 0.05142 sec - 401.70 sec/sample)\n",
      "Early stop after 3 batch(es)\n"
     ]
    }
   ],
   "source": [
    "## TEST\n",
    "\n",
    "fashion_mnsit_inference(\"./models/fashion_mnsit_quant_INT8_edgetpu.tflite\", target_tpu=True, batch_size=batch_size, batch_limit=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ./models/fashion_mnsit.tflite\n",
      "Batch size: 128\n",
      "\n",
      "Model's Input details:\n",
      "[{'dtype': <class 'numpy.float32'>,\n",
      "  'index': 18,\n",
      "  'name': 'input_1',\n",
      "  'quantization': (0.0, 0),\n",
      "  'shape': array([128,  28,  28,   1], dtype=int32)}]\n",
      "\n",
      "Model's Output details:\n",
      "[{'dtype': <class 'numpy.float32'>,\n",
      "  'index': 0,\n",
      "  'name': 'Identity',\n",
      "  'quantization': (0.0, 0),\n",
      "  'shape': array([ 1, 10], dtype=int32)}]\n",
      "\n",
      "batch   1 (batch size 128): accuracy=90.62% - Global Accuracy=90.62% (Inference time 0.17294 sec - 1351.10 sec/sample)\n",
      "batch   2 (batch size 128): accuracy=92.19% - Global Accuracy=91.41% (Inference time 0.13290 sec - 1038.31 sec/sample)\n",
      "batch   3 (batch size 128): accuracy=95.31% - Global Accuracy=92.71% (Inference time 0.12306 sec - 961.41 sec/sample)\n",
      "Early stop after 3 batch(es)\n"
     ]
    }
   ],
   "source": [
    "fashion_mnsit_inference(\"./models/fashion_mnsit.tflite\", batch_size=batch_size, batch_limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ./models/fashion_mnsit_quant.tflite\n",
      "Batch size: 128\n",
      "\n",
      "Model's Input details:\n",
      "[{'dtype': <class 'numpy.float32'>,\n",
      "  'index': 18,\n",
      "  'name': 'input_1',\n",
      "  'quantization': (0.0, 0),\n",
      "  'shape': array([128,  28,  28,   1], dtype=int32)}]\n",
      "\n",
      "Model's Output details:\n",
      "[{'dtype': <class 'numpy.float32'>,\n",
      "  'index': 0,\n",
      "  'name': 'Identity',\n",
      "  'quantization': (0.0, 0),\n",
      "  'shape': array([ 1, 10], dtype=int32)}]\n",
      "\n",
      "batch   1 (batch size 128): accuracy=89.84% - Global Accuracy=89.84% (Inference time 0.18021 sec - 1407.89 sec/sample)\n",
      "batch   2 (batch size 128): accuracy=92.19% - Global Accuracy=91.02% (Inference time 0.16977 sec - 1326.32 sec/sample)\n",
      "batch   3 (batch size 128): accuracy=95.31% - Global Accuracy=92.45% (Inference time 0.17185 sec - 1342.56 sec/sample)\n",
      "Early stop after 3 batch(es)\n"
     ]
    }
   ],
   "source": [
    "fashion_mnsit_inference(\"./models/fashion_mnsit_quant.tflite\", batch_size=batch_size, batch_limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ./models/fashion_mnsit_quant_size.tflite\n",
      "Batch size: 128\n",
      "\n",
      "Model's Input details:\n",
      "[{'dtype': <class 'numpy.float32'>,\n",
      "  'index': 18,\n",
      "  'name': 'input_1',\n",
      "  'quantization': (0.0, 0),\n",
      "  'shape': array([128,  28,  28,   1], dtype=int32)}]\n",
      "\n",
      "Model's Output details:\n",
      "[{'dtype': <class 'numpy.float32'>,\n",
      "  'index': 0,\n",
      "  'name': 'Identity',\n",
      "  'quantization': (0.0, 0),\n",
      "  'shape': array([ 1, 10], dtype=int32)}]\n",
      "\n",
      "batch   1 (batch size 128): accuracy=89.84% - Global Accuracy=89.84% (Inference time 0.18317 sec - 1430.99 sec/sample)\n",
      "batch   2 (batch size 128): accuracy=92.19% - Global Accuracy=91.02% (Inference time 0.18257 sec - 1426.35 sec/sample)\n",
      "batch   3 (batch size 128): accuracy=95.31% - Global Accuracy=92.45% (Inference time 0.17980 sec - 1404.71 sec/sample)\n",
      "Early stop after 3 batch(es)\n"
     ]
    }
   ],
   "source": [
    "fashion_mnsit_inference(\"./models/fashion_mnsit_quant_size.tflite\", batch_size=batch_size, batch_limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ./models/fashion_mnsit_quant_latency.tflite\n",
      "Batch size: 128\n",
      "\n",
      "Model's Input details:\n",
      "[{'dtype': <class 'numpy.float32'>,\n",
      "  'index': 18,\n",
      "  'name': 'input_1',\n",
      "  'quantization': (0.0, 0),\n",
      "  'shape': array([128,  28,  28,   1], dtype=int32)}]\n",
      "\n",
      "Model's Output details:\n",
      "[{'dtype': <class 'numpy.float32'>,\n",
      "  'index': 0,\n",
      "  'name': 'Identity',\n",
      "  'quantization': (0.0, 0),\n",
      "  'shape': array([ 1, 10], dtype=int32)}]\n",
      "\n",
      "batch   1 (batch size 128): accuracy=89.84% - Global Accuracy=89.84% (Inference time 0.17634 sec - 1377.69 sec/sample)\n",
      "batch   2 (batch size 128): accuracy=92.19% - Global Accuracy=91.02% (Inference time 0.17045 sec - 1331.65 sec/sample)\n",
      "batch   3 (batch size 128): accuracy=95.31% - Global Accuracy=92.45% (Inference time 0.17025 sec - 1330.06 sec/sample)\n",
      "Early stop after 3 batch(es)\n"
     ]
    }
   ],
   "source": [
    "fashion_mnsit_inference(\"./models/fashion_mnsit_quant_latency.tflite\", batch_size=batch_size, batch_limit=3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual-env",
   "language": "python",
   "name": "virtual-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
